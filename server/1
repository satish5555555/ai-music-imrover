import os
import torch
import torchaudio
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from pathlib import Path

# ===============================================================
# High-quality audio loader — NO LENGTH LIMIT
# ===============================================================
def load_audio_high_quality(file_path, target_sr=32000, max_len=None):
    """
    Load, resample, normalize full audio track.
    No trimming or length restriction when max_len=None.
    """
    waveform, sr = torchaudio.load(file_path)

    # Convert stereo to mono if needed
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # Resample if needed
    if sr != target_sr:
        waveform = torchaudio.functional.resample(waveform, sr, target_sr)

    # Normalize amplitude
    waveform = waveform / waveform.abs().max().clamp_min(1e-6)

    # Optional limit (disabled for full training)
    if max_len is not None:
        max_samples = int(target_sr * max_len)
        if waveform.shape[1] > max_samples:
            waveform = waveform[:, :max_samples]
        else:
            pad_len = max_samples - waveform.shape[1]
            waveform = F.pad(waveform, (0, pad_len))

    return waveform, target_sr


# ===============================================================
# Simple Conv1D Autoencoder for audio enhancement
# ===============================================================
class AudioAutoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv1d(1, 16, kernel_size=9, stride=2, padding=4),
            nn.ReLU(),
            nn.Conv1d(16, 32, kernel_size=9, stride=2, padding=4),
            nn.ReLU(),
            nn.Conv1d(32, 64, kernel_size=9, stride=2, padding=4),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(64, 32, kernel_size=9, stride=2, padding=4, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose1d(32, 16, kernel_size=9, stride=2, padding=4, output_padding=1),
            nn.ReLU(),
            nn.Conv1d(16, 1, kernel_size=9, stride=1, padding=4),
            nn.Tanh()
        )

    def forward(self, x):
        z = self.encoder(x)
        out = self.decoder(z)
        return out


# ===============================================================
# TRAINING FUNCTION (Full-length)
# ===============================================================
def train_model(data_dir="/app/server/uploads", mode="music", epochs=5, lr=1e-4, target_sr=32000):
    """
    Train on full songs — no cropping, no length limit.
    """
    print(f"[TRAIN] Loading full-length audio from {data_dir}")
    data_dir = Path(data_dir)
    audio_files = list(data_dir.glob("*.wav")) + list(data_dir.glob("*.mp3"))
    if not audio_files:
        raise RuntimeError("No audio files found for training")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[TRAIN] Using device: {device}")

    model = AudioAutoencoder().to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        for f in audio_files:
            waveform, sr = load_audio_high_quality(f, target_sr, max_len=None)
            x = waveform.unsqueeze(0).to(device)

            # Add small noise for denoising
            noisy = x + 0.02 * torch.randn_like(x)
            out = model(noisy)

            # Align lengths
            if out.shape[2] > x.shape[2]:
                out = out[:, :, :x.shape[2]]
            elif out.shape[2] < x.shape[2]:
                pad_len = x.shape[2] - out.shape[2]
                out = F.pad(out, (0, pad_len))

            loss = loss_fn(out, x)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(audio_files)
        print(f"[TRAIN] Epoch {epoch+1}/{epochs}, loss={avg_loss:.6f}")

    ckpt_path = Path(__file__).parent / "checkpoints" / "trained_model.pt"
    os.makedirs(ckpt_path.parent, exist_ok=True)
    torch.save(model.state_dict(), ckpt_path)
    print(f"[TRAIN] ✅ Saved model to {ckpt_path}")


# ===============================================================
# INFERENCE FUNCTION (Full-length, high-fidelity safe export)
# ===============================================================
def improve_music_or_audio(input_path, output_path, mode="auto", target_sr=32000):
    """
    Enhance or reconstruct a full audio track and save it safely (WAV or MP3).
    """
    print(f"[IMPROVE] Enhancing {input_path} -> {output_path}")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AudioAutoencoder().to(device)
    ckpt_path = Path(__file__).parent / "checkpoints" / "trained_model.pt"
    if not ckpt_path.exists():
        raise RuntimeError("No trained model found. Please run training first.")
    model.load_state_dict(torch.load(ckpt_path, map_location=device))
    model.eval()

    waveform, sr = load_audio_high_quality(input_path, target_sr, max_len=None)

    with torch.no_grad():
        enhanced = model(waveform.unsqueeze(0).to(device))
    enhanced = enhanced.squeeze(0).cpu()

    # Match length exactly
    if enhanced.shape[1] > waveform.shape[1]:
        enhanced = enhanced[:, :waveform.shape[1]]
    elif enhanced.shape[1] < waveform.shape[1]:
        pad_len = waveform.shape[1] - enhanced.shape[1]
        enhanced = F.pad(enhanced, (0, pad_len))

    # Normalize + convert to 16-bit PCM
    enhanced = enhanced / enhanced.abs().max().clamp_min(1e-6)
    enhanced = (enhanced * 32767.0).short()

    os.makedirs(Path(output_path).parent, exist_ok=True)
    ext = Path(output_path).suffix.lower()
    temp_wav = str(Path(output_path).with_suffix(".wav"))

    # Save as WAV first
    torchaudio.save(temp_wav, enhanced, target_sr, format="wav")

    # If output should be MP3, convert via ffmpeg (for playable export)
    if ext == ".mp3":
        os.system(f"ffmpeg -y -i {temp_wav} -codec:a libmp3lame -b:a 320k {output_path} >/dev/null 2>&1")
        os.remove(temp_wav)
    else:
        if not output_path.endswith(".wav"):
            os.rename(temp_wav, output_path)

    print(f"[IMPROVE] ✅ Saved enhanced audio to {output_path}")

